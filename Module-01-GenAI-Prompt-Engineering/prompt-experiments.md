# Prompt Experiments – Module 1

The goal of this file is to observe how prompt structure affects LLM output.

---

## Experiment 1 – Basic Prompt

### Prompt
Explain recursion.

### Observation
- Structured
- Clean
- Technically solid
- Balanced depth
- Neutral tone

---

## Experiment 2 – Structured Prompt

### Prompt
Explain recursion to a beginner using:
- Simple language
- A real-world analogy
- Step-by-step explanation
- One small code example

### Observation
- Much more layered
- More engaging
- Controlled structure
- Clear sections
- Beginner friendly
- Teaching style changed

---

## Observed Insight

The structured prompt produced more controlled and layered output compared to the generic prompt.

Key observation:
The more constraints and structure provided in the prompt, the more predictable and usable the output becomes.

Prompt engineering is not about fancy wording.
It is about controlling output format, reasoning style, and target audience.

---

## Experiment 3 – Role-Based Prompt

### Prompt
You are a senior software engineer mentoring a junior developer.  
Explain recursion clearly with practical insight.

### Observation
- Result → Perspective shift + domain maturity.

---

## Experiment 3 Insight – Role-Based Prompting

Adding a role (e.g., "senior software engineer mentoring a junior") significantly changed:

- Tone
- Practical depth
- Real-world warnings
- Interview-oriented insights

Observation:
Role-based prompting influences perspective and domain maturity of responses.

Conclusion:
Prompt engineering is not only about structure.
It is also about assigning perspective and authority to the model.
